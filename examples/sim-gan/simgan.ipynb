{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "sys.path.insert(0,'../..')\n",
    "\n",
    "from AutoGAN import GAN\n",
    "from AutoGAN.schemes.CycleGAN_TrainingScheme import CycleWGAN_TrainingScheme, CycleGAN_TrainingScheme\n",
    "from AutoGAN.schemes.SimGAN_TrainingScheme import SimGAN_TrainingScheme\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import UpSampling2D, LeakyReLU, Lambda, Add, Multiply, Activation, Conv2DTranspose\n",
    "from keras.layers import Cropping2D, ZeroPadding2D, Flatten, Subtract, Input, add, multiply\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from skimage.transform import resize\n",
    "import glob\n",
    "from random import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "config.gpu_options.visible_device_list = \"1\"\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "def build_generator(size):\n",
    "    \"\"\"\n",
    "    The refiner network, Rθ, is a residual network (ResNet). It modifies the synthetic image on a pixel level, rather\n",
    "    than holistically modifying the image content, preserving the global structure and annotations.\n",
    "    :param input_image_tensor: Input tensor that corresponds to a synthetic image.\n",
    "    :return: Output tensor that corresponds to a refined synthetic image.\n",
    "    \"\"\"\n",
    "    def resnet_block(input_features, nb_features=64, nb_kernel_rows=3, nb_kernel_cols=3):\n",
    "        \"\"\"\n",
    "        A ResNet block with two `nb_kernel_rows` x `nb_kernel_cols` convolutional layers,\n",
    "        each with `nb_features` feature maps.\n",
    "        See Figure 6 in https://arxiv.org/pdf/1612.07828v1.pdf.\n",
    "        :param input_features: Input tensor to ResNet block.\n",
    "        :return: Output tensor from ResNet block.\n",
    "        \"\"\"\n",
    "        y = Conv2D(nb_features, (nb_kernel_rows,nb_kernel_cols), padding='same')(input_features)\n",
    "        y = Activation('relu')(y)\n",
    "        y = Conv2D(nb_features, (nb_kernel_rows,nb_kernel_cols), padding='same')(y)\n",
    "\n",
    "        y = add([input_features, y])\n",
    "        return Activation('relu')(y)\n",
    "\n",
    "    # an input image of size w × h is convolved with 3 × 3 filters that output 64 feature maps\n",
    "    img = Input(shape=size)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(img)\n",
    "\n",
    "    # the output is passed through 4 ResNet blocks\n",
    "    for _ in range(4):\n",
    "        x = resnet_block(x)\n",
    "\n",
    "    # the output of the last ResNet block is passed to a 1 × 1 convolutional layer producing 1 feature map\n",
    "    # corresponding to the refined synthetic image\n",
    "    x = Conv2D(1, (1, 1), padding='same', activation='tanh')(x)\n",
    "    return Model(img, x)\n",
    "\n",
    "\n",
    "def build_discriminator(size):\n",
    "    \"\"\"\n",
    "    The discriminator network, Dφ, contains 5 convolution layers and 2 max-pooling layers.\n",
    "    :param input_image_tensor: Input tensor corresponding to an image, either real or refined.\n",
    "    :return: Output tensor that corresponds to the probability of whether an image is real or refined.\n",
    "    \"\"\"\n",
    "    img = Input(shape=size)\n",
    "    x = Conv2D(96, (3, 3), padding='same', strides=2, activation='relu')(img)\n",
    "    x = Conv2D(64, (3, 3), padding='same', strides=2, activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), padding='same', strides=(1, 1))(x)\n",
    "    x = Conv2D(32, (3, 3), padding='same', strides=1, activation='relu')(x)\n",
    "    x = Conv2D(32, (1, 1), padding='same', strides=1, activation='relu')(x)\n",
    "    x = Conv2D(1, (1, 1), padding='same', strides=1, activation='sigmoid')(x)\n",
    "    # here one feature map corresponds to `is_real`\n",
    "    return Model(img, x)\n",
    "\n",
    "\n",
    "def butchered_mp_normalized_matlab_helper(mat_file_path):\n",
    "    \"\"\"\n",
    "    Normalized data is provided in matlab files in MPIIGaze Dataset and these are tricky to load with Python.\n",
    "    This function was made with guessing and checking. Very frustrating.\n",
    "    :param mat_file_path: Full path to MPIIGaze Dataset matlab file.\n",
    "    :return: np array of images.\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    import os\n",
    "    import uuid\n",
    "\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import scipy.io as sio\n",
    "\n",
    "    \n",
    "    x = sio.loadmat(mat_file_path)\n",
    "    y = x.get('data')\n",
    "    z = y[0, 0]\n",
    "\n",
    "    left_imgs = z['left']['image'][0, 0]\n",
    "    right_imgs = z['right']['image'][0, 0]\n",
    "\n",
    "    for img in np.concatenate((left_imgs, right_imgs)):\n",
    "        Image.fromarray(img).save(os.path.join('./RealGaze_data', '{}.png'.format(uuid.uuid4())))\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_batch(image_batch, figure_path, label_batch=None, vmin=0, vmax=255, scale=True):\n",
    "    \"\"\"\n",
    "    Plots a batch of images and their corresponding label(s)/annotations, saving the plot to disc.\n",
    "    :param image_batch: Batch of images to be plotted.\n",
    "    :param figure_path: Full path of the filename the plot will be saved as.\n",
    "    :param label_batch: Batch of labels corresponding to `image_batch`.\n",
    "       Labels will be displayed along w/ their corresponding image.\n",
    "    \"\"\"\n",
    "    if label_batch is not None:\n",
    "        assert len(image_batch) == len(label_batch), 'Their must be a label for each image to be plotted.'\n",
    "\n",
    "    batch_size = len(image_batch)\n",
    "    assert batch_size >= 1\n",
    "\n",
    "    assert isinstance(image_batch, np.ndarray), 'image_batch must be an np array.'\n",
    "\n",
    "    # for gray scale images if image_batch.shape == (img_height, img_width, 1) plt requires this to be reshaped\n",
    "    if image_batch.shape[-1] == 1:\n",
    "        image_batch = np.reshape(image_batch, newshape=image_batch.shape[:-1])\n",
    "\n",
    "    # plot images in rows and columns\n",
    "    # `+ 2` prevents plt.subplots from throwing: `TypeError: 'AxesSubplot' object does not support indexing` when batch_size < 10\n",
    "    nb_rows = batch_size // 3\n",
    "    nb_columns = 3\n",
    "    \n",
    "    import matplotlib\n",
    "    matplotlib.rcParams.update({'axes.titlesize': 10})\n",
    "    _, axs = plt.subplots(nb_rows, nb_columns, figsize=(5, 10))\n",
    "    cnt = 0 \n",
    "    for i in range(nb_columns):\n",
    "        for j in range(nb_rows):\n",
    "            try:\n",
    "                axs[j,i].imshow((image_batch[cnt]+1.)/2., cmap='gray')\n",
    "                if label_batch is not None:\n",
    "                    if j == 0:\n",
    "                        axs[j, i].set_title(label_batch[cnt])\n",
    "                    cnt += 1\n",
    "                axs[j,i].axis('off')\n",
    "            except IndexError:\n",
    "                break\n",
    "\n",
    "    plt.savefig(os.path.join(figure_path))\n",
    "    plt.close()\n",
    "\n",
    "def load_data_h5():\n",
    "    import h5py\n",
    "    with h5py.File('./gaze.h5','r') as t_file:\n",
    "        #print(list(t_file.keys()))\n",
    "        assert 'image' in t_file, \"Images are missing\"\n",
    "        assert 'look_vec' in t_file, \"Look vector is missing\"\n",
    "        assert 'path' in t_file, \"Paths are missing\"\n",
    "        #print('Synthetic images found:',len(t_file['image']))\n",
    "        for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n",
    "            #print('image',ikey,'shape:',ival.shape)\n",
    "            img_height, img_width = ival.shape\n",
    "            img_channels = 1\n",
    "        syn_image_stack = np.stack([np.expand_dims(a,-1) for a in t_file['image'].values()],0)\n",
    "    print(syn_image_stack.shape)\n",
    "    \n",
    "    with h5py.File('./real_gaze.h5','r') as t_file:\n",
    "        #print(list(t_file.keys()))\n",
    "        assert 'image' in t_file, \"Images are missing\"\n",
    "        #print('Real Images found:',len(t_file['image']))\n",
    "        for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n",
    "            #print('image',ikey,'shape:',ival.shape)\n",
    "            img_height, img_width = ival.shape\n",
    "            img_channels = 1\n",
    "        real_image_stack = np.stack([np.expand_dims(a,-1) for a in t_file['image'].values()],0)\n",
    "    print(real_image_stack.shape)\n",
    "    A_list, B_list = [i for i in range(syn_image_stack.shape[0])], [i for i in range(real_image_stack.shape[0])]\n",
    "    shuffle(A_list)\n",
    "    shuffle(B_list)\n",
    "    A = syn_image_stack[A_list]\n",
    "    B = real_image_stack[B_list]\n",
    "    return (2. * A/255.) - 1., (2. * B/255.) - 1.\n",
    "\n",
    "class save_images(keras.callbacks.Callback):\n",
    "    def __init__(self, model, A, B, freq, dataset):\n",
    "        super(save_images, self).__init__()\n",
    "        try:\n",
    "            shutil.rmtree('images/%s'%dataset)\n",
    "        except:\n",
    "            pass        \n",
    "        try:\n",
    "            os.makedirs('images/%s'%dataset)\n",
    "        except:\n",
    "            pass\n",
    "        self.full_model = model\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.epoch = 0\n",
    "        self.freq = freq\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "        #print('started epoch %d' % epoch)\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if batch % self.freq == 0:\n",
    "            preds = self.full_model.generator_model().predict_on_batch(self.A)\n",
    "            gen_imgs = np.array([self.A[i, :, :, 0] for i in range(8)] + \n",
    "                                [preds[i, :, :, 0] for i in range(8)] + \n",
    "                                [self.B[i, :, :, 0] for i in range(8)])\n",
    "            gen_labels = [\"Synthetic\"] * 8 + [\"Refined\"] *8 + [\"Real\"] * 8\n",
    "            plot_batch(gen_imgs, \"images/%s/sim_%d_%d.png\" % (self.dataset, self.epoch, batch), gen_labels)\n",
    "            #print('\\n sampled data at epoch %d , batch %d' % (self.epoch, batch))\n",
    "            #print(logs)\n",
    "            #print('\\n')\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        for i in range(0, A.shape[0]-8, 8):\n",
    "            preds = self.full_model.generator_model().predict_on_batch(self.A)\n",
    "            gen_imgs = np.concatenate([self.A[i:i+8, :, :, 0] ,preds[i:i+8, :, :, 0], self.B[i:i+8, :, :, 0]])\n",
    "            gen_labels = [\"Synthetic\"] * 8 + [\"Refined\"] * 8 + [\"Real\"] * 8\n",
    "            plot_batch(gen_imgs, \"images/%s/sim_final_%d.png\" % (self.dataset, i), gen_labels)\n",
    "\n",
    "def simgan(image_A, image_B):\n",
    "    model = GAN(generator=build_generator(image_A.shape), \n",
    "                discriminator=build_discriminator(image_A.shape))\n",
    "    optimizer = keras.optimizers.SGD(lr=1e-3)\n",
    "    discriminator_kwargs = {'loss':'binary_crossentropy', 'metrics':['accuracy'], 'optimizer': optimizer}\n",
    "    generator_kwargs = {'generator_loss':'mae', 'optimizer': optimizer,\n",
    "                        'discriminator_loss':'binary_crossentropy', #'generator_metrics':['mae'],\n",
    "                        'discriminator_loss_weight':1, 'generator_loss_weight':1}\n",
    "    \n",
    "    model.compile(training_scheme=SimGAN_TrainingScheme(100 * 512),\n",
    "                  generator_kwargs=generator_kwargs, discriminator_kwargs=discriminator_kwargs)\n",
    "    return model\n",
    "\n",
    "      \n",
    "            \n",
    "class pretrain_model(keras.callbacks.Callback):\n",
    "    def __init__(self, my_model, x, y, epochs, batch_size, loss, metrics, optimizer):        \n",
    "        self.my_model = my_model\n",
    "        self.x, self.y = x, y\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.optimizer = optimizer\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.my_model.compile(loss=self.loss, metrics=self.metrics, optimizer=self.optimizer)\n",
    "        self.my_model.fit(self.x, self.y, epochs=self.epochs, batch_size=self.batch_size, verbose=1, shuffle=True, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = load_data_h5()\n",
    "A_test, B_test = A[-1000:], B[-1000:]\n",
    "A, B = A[:-1000], B[:-1000]\n",
    "\n",
    "model = simgan(A[0], B[0])\n",
    "#model.summary()\n",
    "%matplotlib inline\n",
    "out_shape = list(model.discriminator_model().output_shape[1:])\n",
    "\n",
    "pre_train_gen = pretrain_model(model.generator_model(), A[:4096], A[:4096], 5, 64, 'mae', ['mae'], keras.optimizers.Adam(lr=0.0001))\n",
    "pre_train_dis = pretrain_model(model.discriminator_model(), np.concatenate([A[:4096], B[:4096]]),\n",
    "                               np.concatenate([np.zeros([4096]+out_shape), np.ones([4096]+out_shape)]), \n",
    "                               5, 32, 'binary_crossentropy', ['accuracy'], keras.optimizers.Adam(lr=0.0001))\n",
    "model.fit(x=A, y=B, epochs=10, steps_per_epoch=1000, batch_size=512,\n",
    "          generator_callbacks=[pre_train_gen, pre_train_dis, save_images(model, A_test, B_test, 100,'simeyes')], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF 1.12 Keras 2.2.4",
   "language": "python",
   "name": "tf-1.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
